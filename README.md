# GLTCH-2.7M

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—  â–ˆâ–ˆâ•—     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—  â•‘
â•‘   â–ˆâ–ˆâ•”â•â•â•â•â• â–ˆâ–ˆâ•‘  â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘     â•šâ•â•â•â•â–ˆâ–ˆâ•—   â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘  â•‘
â•‘   â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•       â–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘  â•‘
â•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•”â•â•â•â•       â–ˆâ–ˆâ•”â• â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘  â•‘
â•‘   â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘  â•‘
â•‘    â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â•â•â•šâ•â•    â•šâ•â•â•â•â•â•â•šâ•â•  â•šâ•â•     â•šâ•â•â•â•â•â•â•â•šâ•â•   â•šâ•â•  â•šâ•â•     â•šâ•â•  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**Generative Language Transformer with Contextual Hierarchy**

A 2.7 million parameter language model built from scratch, with distributed training support via the GLTCH Hive network.

## Features

- ğŸ§  **Complete transformer architecture** â€” Self-attention, multi-head attention, feedforward networks
- ğŸ“Š **2.7M parameters** â€” Small enough to train on free Google Colab GPUs
- ğŸŒ **Distributed training** â€” GLTCH Hive allows peers to contribute GPU power
- ğŸ¨ **Visual dashboard** â€” Animated node visualization of the training network

## Quick Start

### Train Locally (Single GPU)

```bash
# Clone the repo
git clone https://github.com/cyberdreadx/gltch-2.7m.git
cd gltch-2.7m

# Install dependencies
pip install torch requests

# Train the model
python gltch_2_7m.py
```

### Train on Google Colab (Free GPU)

1. Open [Google Colab](https://colab.research.google.com)
2. Upload `gltch_2_7m_colab.py`
3. Go to **Runtime â†’ Change runtime type â†’ T4 GPU**
4. Run each cell in order

Training takes ~5 minutes on a T4 GPU.

## GLTCH Hive â€” Distributed Training

Contribute GPU power to the hive or run your own training network.

### Start the Coordinator

```bash
cd hive
pip install websockets
python server.py
```

Dashboard available at: http://localhost:8080

### Join as a Peer

```bash
python hive/peer.py --server ws://localhost:8765 --name my-node
```

## Architecture

```
GLTCH-2.7M
â”œâ”€â”€ Token Embedding (65 Ã— 192)
â”œâ”€â”€ Position Embedding (128 Ã— 192)
â”œâ”€â”€ 6Ã— Transformer Blocks
â”‚   â”œâ”€â”€ Multi-Head Attention (6 heads)
â”‚   â”œâ”€â”€ Layer Norm
â”‚   â”œâ”€â”€ Feed Forward (192 â†’ 768 â†’ 192)
â”‚   â””â”€â”€ Layer Norm
â”œâ”€â”€ Final Layer Norm
â””â”€â”€ Output Head (192 â†’ 65)
```

| Component | Size |
|-----------|------|
| Parameters | 2,708,736 |
| Context Length | 128 tokens |
| Embedding Dim | 192 |
| Attention Heads | 6 |
| Layers | 6 |

## Project Structure

```
gltch-2.7m/
â”œâ”€â”€ gltch_2_7m.py          # Main model (single file)
â”œâ”€â”€ gltch_2_7m_colab.py    # Colab version with cells
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â””â”€â”€ hive/                  # Distributed training
    â”œâ”€â”€ index.html         # Dashboard
    â”œâ”€â”€ style.css          # Dark theme
    â”œâ”€â”€ hive.js            # Node visualization
    â”œâ”€â”€ server.py          # Coordinator
    â””â”€â”€ peer.py            # Training peer
```

## Requirements

- Python 3.8+
- PyTorch 2.0+
- websockets (for Hive only)

## License

MIT License â€” see [LICENSE](LICENSE)

## Author

Created by **cyberdreadx**

---

*GLTCH â€” Generative Language Transformer with Contextual Hierarchy*
